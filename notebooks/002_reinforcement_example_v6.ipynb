{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../dependencies/BristolStockExchange'))\n",
    "sys.path.append(os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model characteristics:\n",
    "* Continuous inputs, discrete actions\n",
    "* Discrete events in continuous time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flappy Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* State: (hight, lower border of the hole, upper border of the hole, time to hole)\n",
    "* Actions: {fly up, keep falling}\n",
    "* Reward: time survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reinforcement_example.game import Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play the game\n",
    "game = Game()\n",
    "while True:\n",
    "    print(game.get_state())\n",
    "    action = int(input())\n",
    "    game.time_step(action)\n",
    "    if not game.alive:\n",
    "        print(\"DEAD!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reinforcement_example.learner import make_model, make_model_elu, KernelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reinforcement_example.learner import AbstractLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reinforcement_example.learner import ScaledModel, ValueToPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reinforcement_trading.tools import logodds_to_probs, weights_to_inds, random_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class SoftPolicy(object):\n",
    "#     def __init__(self, base, prob_random):\n",
    "#         self.base = base\n",
    "#         self.p = prob_random\n",
    "        \n",
    "#     def predict(self, states):\n",
    "#         res = self.base.predict(states)\n",
    "#         num_actions = res.shape[1]\n",
    "#         uniform = np.ones(num_actions) / num_actions\n",
    "#         res = self.p * uniform + (1 - self.p) * res\n",
    "#         return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TDLearner(AbstractLearner):\n",
    "    def __init__(self, state_dim, num_actions, reward_scale=1.0, model_factory=make_model_elu):\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.reward_scale = reward_scale\n",
    "        self.value_model = [ScaledModel(model_factory(state_dim, 1), yscale=reward_scale) for i in range(num_actions)]\n",
    "        #self.policy = make_model(state_dim, num_actions)\n",
    "        self.policy = ValueToPolicy(self.value_model, scale=self.reward_scale)\n",
    "        self.histories = []\n",
    "        self.history = []\n",
    "        # Helper arrays for faster training\n",
    "        self.states = np.empty((0, self.state_dim))\n",
    "        self.actions = np.empty(0, dtype=np.int)\n",
    "        self.rewards = np.empty(0)\n",
    "        self.logprobs = np.empty(0)\n",
    "        self.ranges = np.empty((0, 2), dtype=np.int)\n",
    "        self.actinds = [np.empty(0, dtype=np.int) for i in range(self.num_actions)]\n",
    "\n",
    "    def move(self, state):\n",
    "        probs = logodds_to_probs(self.policy.predict(state[np.newaxis, :])[0])\n",
    "        choice = np.searchsorted(np.cumsum(probs), np.random.rand())\n",
    "        self.last_action_prob = probs[choice]\n",
    "        return choice\n",
    "    \n",
    "    def _state_value(self, state):\n",
    "        action_values = np.array([model.predict(state[np.newaxis, :])[0, 0] for model in self.value_model])\n",
    "        probs = logodds_to_probs(self.policy.predict(state[np.newaxis, :])[0, :])\n",
    "        return np.dot(probs, action_values)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, value_proxy):\n",
    "        winds = [0] #weights_to_inds(np.array([self.last_action_prob]))\n",
    "#         self.value_model[action].fit(state[np.newaxis, :][winds, :],\n",
    "#                                      np.array([[reward + self._state_value(next_state)]])[winds],\n",
    "#                                      verbose=False)\n",
    "        self.history.append((np.array(state), action, reward, self.last_action_prob))\n",
    "\n",
    "    def learn_last(self, state, action, reward):\n",
    "        winds = [0] #weights_to_inds(np.array([self.last_action_prob]))\n",
    "#         self.value_model[action].fit(state[np.newaxis, :][winds, :],\n",
    "#                                      np.array([[reward]])[winds],\n",
    "#                                      verbose=False)\n",
    "        self.history.append((np.array(state), action, reward, self.last_action_prob))\n",
    "        self.histories.append(self.history)\n",
    "        self.history = []\n",
    "        self._update_value_model()\n",
    "\n",
    "    def _update_value_model(self):\n",
    "        # Update data structures\n",
    "        dsize = len(self.states)\n",
    "        for i in range(len(self.ranges), len(self.histories)):\n",
    "            last_states, last_actions, last_rewards, last_actionprob = zip(*self.histories[i][::-1])\n",
    "            self.ranges = np.concatenate([self.ranges, np.array([[dsize, dsize + len(last_states)]])])\n",
    "            dsize += len(last_states)\n",
    "            self.states = np.concatenate([self.states, np.array(last_states)], axis=0)\n",
    "            self.actions = np.concatenate([self.actions, np.array(last_actions, dtype=np.int)])\n",
    "            self.rewards = np.concatenate([self.rewards, last_rewards])\n",
    "            #self.logprobs = np.concatenate([self.logprobs, [0], np.log(last_actionprob)[:-1]])\n",
    "            #self.actinds = [np.arange(len(self.actions))[self.actions == i] for i in range(self.num_actions)]\n",
    "            self.actinds = [np.concatenate([self.actinds[i],\n",
    "                                            np.arange(dsize - len(last_states), dsize)\n",
    "                                                [self.actions[dsize - len(last_states):dsize] == i]])\n",
    "                            for i in range(self.num_actions)]\n",
    "        # Training\n",
    "        policy_probs = logodds_to_probs(self.policy.predict(self.states))\n",
    "        #policy_logprobs = np.log(policy_probs[np.arange(dsize), self.actions])\n",
    "        #policy_cumlogprobs = np.zeros(dsize)\n",
    "        #for start, end in self.ranges:\n",
    "        #    policy_cumlogprobs[start + 1:end] = np.cumsum(policy_logprobs[start:end - 1])\n",
    "        predicted_state_action_value = np.array([model.predict(self.states)[:, 0] for model in self.value_model]).T\n",
    "        predicted_state_value = np.sum(policy_probs * predicted_state_action_value, axis=1)\n",
    "        response = np.empty(self.rewards.shape)\n",
    "        response[:] = self.rewards\n",
    "        for start, end in self.ranges:\n",
    "            response[start + 1:end] += predicted_state_value[start:end - 1]\n",
    "        predicted_value = predicted_state_action_value[np.arange(dsize), self.actions]\n",
    "        # weights = np.exp(policy_logprobs - self.logprobs)\n",
    "        for action in range(self.num_actions):\n",
    "            inds = self.actinds[action]  #[weights_to_inds(weights[self.actinds[action]])]\n",
    "            if len(inds) > 0:\n",
    "                self.value_model[action].fit(self.states[inds, :], response[inds],\n",
    "                                             epochs=10#,\n",
    "                                             #callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=2)]\n",
    "                                            )\n",
    "        # Policy scale\n",
    "        #mse = np.dot(weights, (predicted_value - response) ** 2) / np.sum(weights)\n",
    "        #self.policy.set_scale(self.reward_scale / len(self.histories) + np.sqrt(mse))\n",
    "        self.policy.set_scale(self.reward_scale / np.sqrt(len(self.histories)))\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reinforcement_example.learner import train_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_model_factory(xscale):\n",
    "    def factory(ninputs, noutputs):\n",
    "        if ninputs != len(xscale):\n",
    "            raise ValueError(\"Expected {} inputs, requested {}\".format(len(xscale), ninputs))\n",
    "        if noutputs != 1:\n",
    "            raise ValueError(\"Expected 1 output, requested {}\".format(noutputs))\n",
    "        return KernelModel(xscale)\n",
    "    return factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game = Game()\n",
    "learner = TDLearner(4, 2, reward_scale=game.mean_time_to_hole, model_factory=kernel_model_factory(2 * np.ones(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner2 = TDLearner(4, 2, reward_scale=game.mean_time_to_hole)\n",
    "learner2.histories = learner.histories\n",
    "learner2.value_model = learner.value_model\n",
    "learner2._update_value_model()\n",
    "learner_bak = learner\n",
    "learner = learner2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    learner._update_value_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    x = np.zeros(len(history))\n",
    "    x[1:] = np.cumsum([info[2] for info in history[:-1]])\n",
    "    y = np.array([info[0][0] for info in history])\n",
    "    gates = dict()\n",
    "    for i in range(len(history)):\n",
    "        gx = x[i] + history[i][0][3]\n",
    "        gl = history[i][0][1]\n",
    "        gu = history[i][0][2]\n",
    "        gates[np.round(gx)] = (gx, gl, gu)\n",
    "    plt.plot(x, y, '-*')\n",
    "    for gx, gl, gu in gates.values():\n",
    "        plt.plot([gx, gx], [0, gl], 'r-')\n",
    "        plt.plot([gx, gx], [gu, 100], 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fooh, foor = train_play(Game(), learner)\n",
    "show_history(fooh)\n",
    "plt.grid(True)\n",
    "foor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    history.append(train_play(Game(), learner)[1])\n",
    "    if len(history) % 100 == 0:\n",
    "        print(len(history))\n",
    "    if os.path.exists(\"002_reinforcement_example_v6_stop\"):\n",
    "        break\n",
    "plt.plot(np.cumsum(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    history.append(train_play(Game(), learner)[1])\n",
    "plt.plot(np.cumsum(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('002_reinforcement_example_v2_learner.pickle', 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#         'histories': learner.histories,\n",
    "#         'value_model': [\n",
    "#             {\n",
    "#                 'config': model.get_config(),\n",
    "#                 'weights': model.get_weights()\n",
    "#             }\n",
    "#             for model in learner.value_model\n",
    "#         ]\n",
    "#     }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('002_reinforcement_example_v6_learner_v1.pickle', 'wb') as f:\n",
    "    pickle.dump(dict([(foo, learner.__dict__[foo]) for foo in learner.__dict__\n",
    "                      if foo != \"value_model\" and foo != 'policy'] +\n",
    "                     [(\"value_model\", [{\n",
    "                         \"xscale\": model.xscale,\n",
    "                         \"yscale\": model.yscale,\n",
    "                         \"config\": model.model.get_config(),\n",
    "                         \"weights\": model.model.get_weights()\n",
    "                     } for model in learner.value_model])]), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.sort(learner.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.zeros((100, 4))\n",
    "for i in range(foo.shape[0]):\n",
    "    foo[i, :4] = Game().get_state()\n",
    "fooy0 = learner.value_model[0].predict(foo).reshape(-1)\n",
    "fooy1 = learner.value_model[1].predict(foo).reshape(-1)\n",
    "plt.plot(foo[:, 3], fooy0, '.')\n",
    "plt.plot(foo[:, 3], fooy1, '.')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(learner.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        cumrewards = np.empty(len(learner.states))\n",
    "        for start, end in learner.ranges:\n",
    "            cumrewards[start:end] = np.cumsum(learner.rewards[start:end])\n",
    "        for action in range(learner.num_actions):\n",
    "            X = learner.states[learner.actinds[action], :]\n",
    "            Y = cumrewards[learner.actinds[action]]\n",
    "            plt.plot(X[:, 3], Y, '.')\n",
    "        for action in range(learner.num_actions):\n",
    "            X = learner.states[learner.actinds[action], :]\n",
    "            Y = cumrewards[learner.actinds[action]]\n",
    "            plt.figure()\n",
    "            plt.plot(learner.value_model[action].predict(X), Y, '.')\n",
    "            #plt.title('Prediction vs response, RMS = {}'.format(learner.value_model[action].evaluate(X[inds], Y[inds])))\n",
    "            plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_heights = np.linspace(0, 100, 1000)\n",
    "foo_states = np.empty((len(foo_heights), 4))\n",
    "foo_states[:, 0] = foo_heights\n",
    "foo_states[:, 1] = 45\n",
    "foo_states[:, 2] = 55\n",
    "foo_states[:, 3] = 1\n",
    "for action in range(learner.num_actions):\n",
    "    foo_v = learner.value_model[action].predict(foo_states)\n",
    "    plt.plot(foo_heights, foo_v, label=\"Value for action {}\".format(action))\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_dists = np.linspace(0, 100)\n",
    "foo_states = np.empty((len(foo_dists), 4))\n",
    "foo_states[:, 0] = 50\n",
    "foo_states[:, 1] = 45\n",
    "foo_states[:, 2] = 55\n",
    "foo_states[:, 3] = foo_dists\n",
    "for action in range(learner.num_actions):\n",
    "    foo_v = learner.value_model[action].predict(foo_states)\n",
    "    plt.plot(foo_dists, foo_v, label=\"Value for action {}\".format(action))\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.policy.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dy = learner.states[:, 0] - 0.5 * (learner.states[:, 1] + learner.states[:, 2])\n",
    "ybins = np.linspace(-100, 100, 30)\n",
    "xbins = np.linspace(-60, 0, 30)\n",
    "totals = np.zeros((len(xbins) + 1, len(ybins) + 1))\n",
    "counts = np.zeros((len(xbins) + 1, len(ybins) + 1))\n",
    "xi = np.searchsorted(xbins, -learner.states[:, 3])\n",
    "yi = np.searchsorted(ybins, dy)\n",
    "for i in range(len(learner.states)):\n",
    "    totals[xi[i], yi[i]] += learner.cumrewards[i]\n",
    "    counts[xi[i], yi[i]] += 1\n",
    "plt.imshow((totals / counts).T,\n",
    "           origin = 'lower',\n",
    "           aspect='auto',\n",
    "           extent=(np.min(xbins), np.max(xbins), np.min(ybins), np.max(ybins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dy = learner.states[:, 0] - 0.5 * (learner.states[:, 1] + learner.states[:, 2])\n",
    "predictions = np.mean([model.predict(learner.states).reshape(-1) for model in learner.value_model], axis=0)\n",
    "ybins = np.linspace(-100, 100, 30)\n",
    "xbins = np.linspace(-60, 0, 30)\n",
    "totals = np.zeros((len(xbins) + 1, len(ybins) + 1))\n",
    "counts = np.zeros((len(xbins) + 1, len(ybins) + 1))\n",
    "xi = np.searchsorted(xbins, -learner.states[:, 3])\n",
    "yi = np.searchsorted(ybins, dy)\n",
    "for i in range(len(learner.states)):\n",
    "    totals[xi[i], yi[i]] += predictions[i]\n",
    "    counts[xi[i], yi[i]] += 1\n",
    "plt.imshow((totals / counts).T,\n",
    "           origin = 'lower',\n",
    "           aspect='auto',\n",
    "           extent=(np.min(xbins), np.max(xbins), np.min(ybins), np.max(ybins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_heatmap(x, y, z, xbins, ybins):\n",
    "    xi = np.searchsorted(xbins, x)\n",
    "    yi = np.searchsorted(ybins, y)\n",
    "    totals = np.zeros((len(xbins) + 1, len(ybins) + 1))\n",
    "    counts = np.zeros((len(xbins) + 1, len(ybins) + 1))\n",
    "    for i in range(len(learner.states)):\n",
    "        totals[xi[i], yi[i]] += z[i]\n",
    "        counts[xi[i], yi[i]] += 1\n",
    "    plt.imshow((totals / counts).T,\n",
    "               origin = 'lower',\n",
    "               aspect='auto',\n",
    "               extent=(np.min(xbins), np.max(xbins), np.min(ybins), np.max(ybins)))\n",
    "    plt.colorbar()\n",
    "    plt.contour(np.linspace(xbins[0], xbins[-1], len(xbins) + 1),\n",
    "                np.linspace(ybins[0], ybins[-1], len(ybins) + 1),\n",
    "                (totals / counts).T,\n",
    "                colors='k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybins = np.linspace(-100, 100, 30)\n",
    "xbins = np.linspace(-60, 0, 30)\n",
    "dy = learner.states[:, 0] - 0.5 * (learner.states[:, 1] + learner.states[:, 2])\n",
    "for action in range(learner.num_actions):\n",
    "    predictions = learner.value_model[action].predict(learner.states).reshape(-1)\n",
    "    plt.figure()\n",
    "    mean_heatmap(-learner.states[:, 3], dy, predictions, xbins, ybins)\n",
    "\n",
    "predictions = logodds_to_probs(learner.policy.predict(learner.states))[:, 1]\n",
    "plt.figure()\n",
    "mean_heatmap(-learner.states[:, 3], dy, predictions, xbins, ybins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(predictions), np.max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dy = learner.states[:, 0] - 0.5 * (learner.states[:, 1] + learner.states[:, 2])\n",
    "plt.plot(-learner.states[:, 3], dy, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Test the ability of the neural network to learn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = make_model(6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.random.rand(6000).reshape((1000, 6)) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in data:\n",
    "    model.train_on_batch(row[np.newaxis, :], row[[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = np.random.rand(600).reshape((100, 6)) * 3\n",
    "fooy = model.predict(foo)\n",
    "plt.plot(foo[:, -1], fooy.reshape(-1), '.')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3.5",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
